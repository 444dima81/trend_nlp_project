import re
import pandas as pd
import emoji
import os
import json
from langdetect import detect
from sqlalchemy import create_engine, text
import sqlalchemy.types as sqltypes
from dotenv import load_dotenv
import logging

# ===================== –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ =====================
logging.basicConfig(level=logging.INFO)
log = logging.getLogger(__name__)

# ===================== –ö–æ–Ω—Ñ–∏–≥ =====================
load_dotenv()
PG_CONN = os.getenv("PG_CONN")

# --- –í—Å–µ —Å—Ç–æ–ø-–ø–∞—Ç—Ç–µ—Ä–Ω—ã ---
STOP_PATTERNS = [
    # —Å—Ç–∞—Ä—ã–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–±—ç–∫–¥–æ—Ä",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ —Ä–∏–∞ –Ω–æ–≤–æ—Å—Ç–∏",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*—á–∏—Ç–∞—Ç—å —Ä–±–∫ –≤ telegram",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–±–æ–ª—å—à–µ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∏.*?—Ä–±–∫",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–∫–∞—Ä—Ç–∏–Ω–∞ –¥–Ω—è.*?—Ä–±–∫",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–¥—Ä—É–≥–∏–µ –≤–∏–¥–µ–æ —ç—Ç–æ–≥–æ –¥–Ω—è.*?—Ä–±–∫",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*—Å–ª–µ–¥–∏—Ç–µ –∑–∞ –Ω–æ–≤–æ—Å—Ç—è–º–∏.*?—Ä–±–∫",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*–ø—Ä—è–º–æ–π —ç—Ñ–∏—Ä",
    r"[\s\Wüêöüîπüì¢ü§ëüëç]*the —ç–∫–æ–Ω–æ–º–∏—Å—Ç",

    # –Ω–æ–≤—ã–µ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ
    r"–ò–ò by AIvengo",
    r"^–û—Ç–ø—Ä–∞–≤(—å|–∏—Ç—å) .*",           # –µ—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "–û—Ç–ø—Ä–∞–≤—å"
    r"—á–∏—Ç–∞–π—Ç–µ –≤ .*",                # –æ–±—Ä–µ–∑–∞–µ–º –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è "–ß–∏—Ç–∞–π—Ç–µ –≤ ..."
    r"‚Äî –≤ –º–∞—Ç–µ—Ä–∏–∞–ª–µ –†–ë–ö",
    r"–§–æ—Ç–æ: .*",
    r"‚Äî —á–∏—Ç–∞–π—Ç–µ –≤ –ø–æ–¥–ø–∏—Å–∫–µ –†–ë–ö",
    r"–°–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏ ‚Äî –≤ —Ç–µ–ª–µ–≥—Ä–∞–º-–∫–∞–Ω–∞–ª–µ –†–ë–ö",
    r"‚Äì –≤ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–µ –†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"
]

# --- –ü–æ–ª–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–µ–π ---
FULL_REMOVE_PATTERNS = [
    r"–ß–∏—Ç–∞–π—Ç–µ —Å–∞–º—ã–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ",
    r"–ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏",
    r"–£—Ç—Ä–µ–Ω–Ω–∏–π –≤—ã–ø—É—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π",
    r"–±–æ–ª—å—à–æ–π —Ä–æ–∑—ã–≥—Ä—ã—à"
]

# --- –†–µ–≥—É–ª—è—Ä–∫–∞ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã ---
REKLAMA_PATTERN = re.compile(
    r"(?i)(?:—Ä–µ–∫–ª–∞–º–∞[\s\.\-:]*–æ–æ–æ|–æ–æ–æ[\s\.\-:]*—Ä–µ–∫–ª–∞–º–∞)"
)


# ===================== –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ =====================

def remove_urls(text):
    return re.sub(r'http\S+|www\S+', '', text)

def remove_mentions(text):
    return re.sub(r'@\w+', '', text)

def remove_emojis(text: str) -> str:
    return emoji.replace_emoji(text, replace='')

def remove_html_tags(text):
    return re.sub(r'<.*?>', '', text)

def remove_stop_phrases(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç –∏–ª–∏ –æ–±—Ä–µ–∑–∞–µ—Ç –ø–æ —Å—Ç–æ–ø-–ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
    for pattern in STOP_PATTERNS:
        text = re.split(pattern, text, flags=re.IGNORECASE)[0]
    return text.strip()

def remove_full_news(text: str) -> bool:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–∫–ª—é—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Å—Ç–æ–ø-—Ñ—Ä–∞–∑—É"""
    if not text:
        return True
    for pattern in FULL_REMOVE_PATTERNS:
        if re.search(pattern, text, flags=re.IGNORECASE):
            return True
    return False

def clean_text(text):
    if not text:
        return ""
    text = remove_urls(text)
    text = remove_mentions(text)
    text = remove_emojis(text)
    text = remove_html_tags(text)
    text = re.sub(r'[\r\n]+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    text = remove_stop_phrases(text)
    return text

def detect_language(text):
    try:
        return detect(text) if len(text.split()) > 2 else None
    except:
        return None


# ===================== –û–±—Ä–∞–±–æ—Ç–∫–∞ DataFrame =====================

def process_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # –£–¥–∞–ª—è–µ–º —Ä–µ–∫–ª–∞–º—É
    df = df[~df['text'].str.contains(REKLAMA_PATTERN, na=False)]

    # –£–¥–∞–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –ø–æ–ª–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
    df = df[~df['text'].apply(remove_full_news)]

    # –ß–∏—Å—Ç–∏–º —Ç–µ–∫—Å—Ç
    df['text_clean'] = df['text'].apply(clean_text)

    # –£–¥–∞–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (<3 —Å–ª–æ–≤)
    df = df[df['text_clean'].str.split().str.len() >= 3]

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫
    df['lang'] = df['text_clean'].apply(detect_language)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∞–∫—Ü–∏–∏ –≤ JSON
    df['reactions'] = df['reactions'].apply(
        lambda d: json.dumps(d, ensure_ascii=False) if isinstance(d, dict) else None
    )

    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –æ—á–∏—â–µ–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
    df = df.drop_duplicates(subset=['text_clean'])
    return df


# ===================== –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è =====================

def main():
    engine = create_engine(PG_CONN)
    query = """
        SELECT channel_id, message_id, date_utc, text, views, reactions
        FROM messages
        WHERE processed = FALSE;
    """
    df = pd.read_sql(query, engine)

    if df.empty:
        log.info("–ù–µ—Ç –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    df_clean = process_df(df)

    dtype_dict = {
        'channel_id': sqltypes.BigInteger(),
        'message_id': sqltypes.BigInteger(),
        'date_utc': sqltypes.DateTime(),
        'text_clean': sqltypes.Text(),
        'lang': sqltypes.String(length=10),
        'views': sqltypes.BigInteger(),
        'reactions': sqltypes.JSON
    }

    df_clean.to_sql('final_db', engine, if_exists='append', index=False, dtype=dtype_dict)
    log.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df_clean)} –∑–∞–ø–∏—Å–µ–π.")


if __name__ == "__main__":
    main()