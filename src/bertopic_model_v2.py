import os
from dotenv import load_dotenv
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
from qdrant_client import QdrantClient
from bertopic import BERTopic

# -------------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ --------------------
load_dotenv()
PG_CONN = os.getenv("PG_CONN")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
COLLECTION_NAME = "telegram_posts_v2"

# -------------------- –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ --------------------
engine = create_engine(PG_CONN)
client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Qdrant --------------------
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Qdrant...")
points = client.scroll(
    collection_name=COLLECTION_NAME,
    with_vectors=True,
    with_payload=True,
    limit=60000
)

vectors = []
texts = []
message_ids = []

for point in points[0]:
    vectors.append(point.vector)
    payload = point.payload or {}
    texts.append(payload.get("text", ""))
    message_ids.append(point.id)

# –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–∞
clean_vectors = []
for v in vectors:
    if isinstance(v, dict):
        # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–∞–∫ {'embedding': [...]}
        # –∏–ª–∏ {'vector': [...]}, –ø–æ–ø—Ä–æ–±—É–π –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        v = v.get("embedding") or v.get("vector") or list(v.values())[0]
    clean_vectors.append(v)

vectors = np.array(clean_vectors, dtype=np.float32)
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectors)} —Ç–æ—á–µ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERTopic")

# -------------------- –û–±—É—á–µ–Ω–∏–µ BERTopic --------------------
print("üß† –û–±—É—á–∞–µ–º BERTopic...")
topic_model = BERTopic(language="multilingual", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(texts, vectors)

# -------------------- DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ --------------------
df_topics = pd.DataFrame({
    "message_id": message_ids,
    "topic": topics,
    "probability": [p.max() if p is not None else 0 for p in probs],
})

# -------------------- –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ clean_posts --------------------
query = """
SELECT
    message_id,
    text_clean,
    date_utc,
    views,
    channel_id,
    social_reactions,
    sentiment,
    confidence,
    k_words
FROM clean_posts;
"""
df_clean = pd.read_sql(query, engine)

# -------------------- –û–±—ä–µ–¥–∏–Ω—è–µ–º --------------------
df_merged = pd.merge(df_topics, df_clean, on="message_id", how="left")

# –ß–∏—Å—Ç–∏–º null
df_merged['social_reactions'] = df_merged['social_reactions'].fillna(0)
df_merged['confidence'] = df_merged['confidence'].fillna(0)
df_merged['views'] = df_merged['views'].fillna(0).astype(int)
df_merged['sentiment'] = df_merged['sentiment'].fillna('neutral')
df_merged['text_clean'] = df_merged['text_clean'].fillna("")

# -------------------- Recency weight --------------------
df_merged['date_utc'] = pd.to_datetime(df_merged['date_utc'])
df_merged['days_ago'] = (pd.Timestamp.now(tz='UTC') - df_merged['date_utc']).dt.days
# –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≤–µ—Å–∞: –ø–æ—Å—Ç—ã —Å—Ç–∞—Ä—à–µ 14 –¥–Ω–µ–π –ø–æ—á—Ç–∏ –Ω–µ —É—á–∏—Ç—ã–≤–∞—é—Ç—Å—è
df_merged['recency_weight'] = np.exp(-df_merged['days_ago'] / 7)
df_merged['weighted_views'] = df_merged['views'] * df_merged['recency_weight']
df_merged['weighted_reactions'] = df_merged['social_reactions'] * df_merged['recency_weight']

# -------------------- –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sentiment –≤ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ --------------------
sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
df_merged['sentiment_score'] = df_merged['sentiment'].map(sentiment_map)
df_merged['weighted_sentiment'] = df_merged['sentiment_score'] * df_merged['recency_weight']

# -------------------- –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ —Å —É—á—ë—Ç–æ–º recency_weight --------------------
topic_summary = (
    df_merged.groupby("topic")
    .agg(
        weighted_views=("weighted_views", "sum"),
        mean_weighted_sentiment=("weighted_sentiment", "mean"),
        mean_weighted_reaction=("weighted_reactions", "mean"),
        mean_confidence=("confidence", "mean"),
        n_posts=("message_id", "count")
    )
    .reset_index()
    .sort_values("weighted_views", ascending=False)
)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä –ø–æ—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
examples = (
    df_merged.groupby("topic")
    .apply(lambda x: x['text_clean'].sample(1).values[0] if not x['text_clean'].empty else "")
    .reset_index(name='example_post')
)
topic_summary = topic_summary.merge(examples, on="topic", how="left")
# -------------------- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV --------------------
topic_summary.to_csv("../data/bertopic_model_v2.csv", index=False)
print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ bertopic_model_v2.csv")

MODEL_PATH = "../models/bertopic_model_v2"
os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
topic_model.save(MODEL_PATH)
print(f"‚úÖ –ú–æ–¥–µ–ª—å BERTopic —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

# –ü–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Qdrant (telegram_posts_v2),
#  ‚Ä¢ –ë–µ—Ä—ë—Ç –≤–µ–∫—Ç–æ—Ä–∞ –∏ —Ç–µ–∫—Å—Ç—ã,
#  ‚Ä¢ –û–±—É—á–∞–µ—Ç BERTopic –Ω–∞ –≥–æ—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–∞—Ö (—É—Å–∫–æ—Ä–µ–Ω–∏–µ √ó3‚Äì5),
#  ‚Ä¢ –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —Ç–∞–±–ª–∏—Ü–µ–π clean_posts,
#  ‚Ä¢ –î–æ–±–∞–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ (views, social_reactions, sentiment).