import os
from dotenv import load_dotenv
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
from qdrant_client import QdrantClient
from bertopic import BERTopic
import matplotlib.pyplot as plt

# -------------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ --------------------
load_dotenv()
PG_CONN = os.getenv("PG_CONN")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
COLLECTION_NAME = "telegram_posts_v2"
MODEL_PATH = "../models/bertopic_model_v3"  # –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏

# -------------------- –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ --------------------
engine = create_engine(PG_CONN)
client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Qdrant --------------------
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Qdrant...")
points = client.scroll(
    collection_name=COLLECTION_NAME,
    with_vectors=True,
    with_payload=True,
    limit=60000
)

vectors = []
texts = []
message_ids = []

for point in points[0]:
    vectors.append(point.vector)
    payload = point.payload or {}
    # –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    texts.append(payload.get("text_clean", "") or payload.get("text", ""))
    message_ids.append(point.id)

# –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–∞
clean_vectors = []
for v in vectors:
    if isinstance(v, dict):
        v = v.get("embedding") or v.get("vector") or list(v.values())[0]
    clean_vectors.append(v)

vectors = np.array(clean_vectors, dtype=np.float32)
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectors)} —Ç–æ—á–µ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERTopic")

# -------------------- –û–±—É—á–µ–Ω–∏–µ BERTopic --------------------
print("üß† –û–±—É—á–∞–µ–º BERTopic...")
topic_model = BERTopic(language="multilingual", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(texts, vectors)

# -------------------- –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å --------------------
topic_model.save(MODEL_PATH)
print(f"‚úÖ –ú–æ–¥–µ–ª—å BERTopic —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

# -------------------- –°–æ–∑–¥–∞—ë–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ --------------------
df_topics = pd.DataFrame({
    "message_id": message_ids,
    "topic": topics,
    "probability": [p.max() if p is not None else 0 for p in probs],
})

# -------------------- –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ clean_posts --------------------
query = """
SELECT
    message_id,
    text_clean,
    date_utc,
    views,
    channel_id,
    social_reactions,
    sentiment,
    confidence,
    k_words
FROM clean_posts;
"""
df_clean = pd.read_sql(query, engine)

# -------------------- –û–±—ä–µ–¥–∏–Ω—è–µ–º --------------------
df_merged = pd.merge(df_topics, df_clean, on="message_id", how="left")
df_merged.fillna({"social_reactions": 0, "confidence": 0, "views": 0, "sentiment": "neutral", "text_clean": ""}, inplace=True)
sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
df_merged['sentiment_score'] = df_merged['sentiment'].map(sentiment_map)

# -------------------- –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ --------------------
topic_summary = (
    df_merged.groupby("topic")
    .agg(
        mean_reaction=("social_reactions", "mean"),
        mean_sentiment=("sentiment_score", "mean"),
        mean_confidence=("confidence", "mean"),
        avg_views=("views", "mean"),
        n_posts=("message_id", "count")
    )
    .reset_index()
    .sort_values("n_posts", ascending=False)
)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä –ø–æ—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
examples = (
    df_merged.groupby("topic")
    .apply(lambda x: x['text_clean'].dropna().sample(1).values[0] if not x['text_clean'].dropna().empty else "")
    .reset_index(name='example_post')
)
topic_summary = topic_summary.merge(examples, on="topic", how="left")

# -------------------- –í—ã–≤–æ–¥ —Ç–æ–ø-15 —Ç–µ–º --------------------
top15 = topic_summary.head(15)
print("üî• –¢–æ–ø-15 —Ç–µ–º:")
print(top15[["topic", "n_posts", "mean_sentiment", "mean_reaction", "example_post"]])

# -------------------- –°–æ—Ö—Ä–∞–Ω—è–µ–º CSV --------------------
topic_summary.to_csv("../data/bertopic_model_v3.csv", index=False)
print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ bertopic_model_v3.csv")

# -------------------- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è UMAP --------------------
umap_emb = topic_model.visualize_topics()
umap_emb.show()

#  ‚Ä¢ –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–æ 60 000 —Ç–æ—á–µ–∫ –∏–∑ Qdrant (–≤–µ–∫—Ç–æ—Ä + payload),
#  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç text_clean –∏ embedding –Ω–∞–ø—Ä—è–º—É—é,
#  ‚Ä¢ –û–±—É—á–∞–µ—Ç BERTopic (—Å –≤–Ω–µ—à–Ω–∏–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏),
#  ‚Ä¢ –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å clean_posts,
#  ‚Ä¢ –í—ã—á–∏—Å–ª—è–µ—Ç:
#  ‚Ä¢ mean_reaction ‚Äî —Å—Ä–µ–¥–Ω–∏–µ —Ä–µ–∞–∫—Ü–∏–∏,
#  ‚Ä¢ mean_sentiment ‚Äî —Å—Ä–µ–¥–Ω–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ,
#  ‚Ä¢ mean_confidence ‚Äî —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–µ,
#  ‚Ä¢ avg_views ‚Äî —Å—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã,
#  ‚Ä¢ n_posts ‚Äî –∫–æ–ª-–≤–æ –ø–æ—Å—Ç–æ–≤ –≤ —Ç–µ–º–µ,
#  ‚Ä¢ –î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ—Å—Ç–∞ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ CSV.