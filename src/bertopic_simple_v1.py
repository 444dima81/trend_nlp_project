import os
from dotenv import load_dotenv
import numpy as np
import pandas as pd
from sqlalchemy import create_engine
from qdrant_client import QdrantClient
from bertopic import BERTopic

# -------------------- –ù–∞—Å—Ç—Ä–æ–π–∫–∞ --------------------
load_dotenv()
PG_CONN = os.getenv("PG_CONN")
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", 6333))
COLLECTION_NAME = "telegram_posts_v2"

# -------------------- –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ --------------------
engine = create_engine(PG_CONN)
client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)

# -------------------- –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Qdrant --------------------
print("üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ Qdrant...")
points = client.scroll(
    collection_name=COLLECTION_NAME,
    with_vectors=True,
    with_payload=True,
    limit=60000  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
)

vectors = []
texts = []
message_ids = []

for point in points[0]:
    vectors.append(point.vector)
    payload = point.payload or {}
    texts.append(payload.get("text", ""))
    message_ids.append(point.id)

# –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä–∞
clean_vectors = []
for v in vectors:
    if isinstance(v, dict):
        # –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä —Ö—Ä–∞–Ω–∏—Ç—Å—è –∫–∞–∫ {'embedding': [...]}
        # –∏–ª–∏ {'vector': [...]}, –ø–æ–ø—Ä–æ–±—É–π –æ–±–∞ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        v = v.get("embedding") or v.get("vector") or list(v.values())[0]
    clean_vectors.append(v)

vectors = np.array(clean_vectors, dtype=np.float32)
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vectors)} —Ç–æ—á–µ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERTopic")

# -------------------- –û–±—É—á–µ–Ω–∏–µ BERTopic --------------------
print("üß† –û–±—É—á–∞–µ–º BERTopic...")
topic_model = BERTopic(language="multilingual", calculate_probabilities=True, verbose=True)
topics, probs = topic_model.fit_transform(texts, vectors)

# -------------------- –°–æ–∑–¥–∞—ë–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ --------------------
df_topics = pd.DataFrame({
    "message_id": message_ids,
    "topic": topics,
    "probability": [p.max() if p is not None else None for p in probs],
})

# -------------------- –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ clean_posts --------------------
query = """
SELECT
    message_id,
    text_clean,
    date_utc,
    views,
    channel_id,
    social_reactions,
    sentiment,
    confidence,
    k_words
FROM clean_posts;
"""
df_clean = pd.read_sql(query, engine)

# -------------------- –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã --------------------
df_merged = pd.merge(df_topics, df_clean, on="message_id", how="left")

# –ß–∏—Å—Ç–∏–º null
df_merged['social_reactions'] = df_merged['social_reactions'].fillna(0)
df_merged['confidence'] = df_merged['confidence'].fillna(0)
df_merged['views'] = df_merged['views'].fillna(0).astype(int)
df_merged['sentiment'] = df_merged['sentiment'].fillna('neutral')

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sentiment –≤ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
sentiment_map = {'negative': -1, 'neutral': 0, 'positive': 1}
df_merged['sentiment_score'] = df_merged['sentiment'].map(sentiment_map)

# -------------------- –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ --------------------
topic_summary = (
    df_merged.groupby("topic")
    .agg(
        mean_reaction=("social_reactions", "mean"),
        mean_sentiment=("sentiment_score", "mean"),
        mean_confidence=("confidence", "mean"),
        avg_views=("views", "mean"),
        n_posts=("message_id", "count")
    )
    .reset_index()
    .sort_values("n_posts", ascending=False)
)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä –ø–æ—Å—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã
examples = (
    df_merged.groupby("topic")
    .apply(lambda x: x['text_clean'].dropna().sample(1).values[0] if not x['text_clean'].dropna().empty else "")
    .reset_index(name='example_post')
)
topic_summary = topic_summary.merge(examples, on="topic", how="left")

MODEL_PATH = "../models/bertopic_model_v1"
os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
topic_model.save(MODEL_PATH)
print(f"‚úÖ –ú–æ–¥–µ–ª—å BERTopic —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

# -------------------- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV --------------------
topic_summary.to_csv("../data/bertopic_model_v1.csv", index=False)
print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ bertopic_model_v1.csv")

# -------------------- –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ PostgreSQL –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É --------------------
topic_summary.to_sql("bert_model_v1", engine, if_exists="replace", index=False)
print("‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ PostgreSQL –≤ —Ç–∞–±–ª–∏—Ü—É bertopic_model_v1")




# –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –Ω–∞–ø—Ä—è–º—É—é –∏–∑ –±–∞–∑—ã.
#  ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π embedding-–º–µ—Ö–∞–Ω–∏–∑–º BERTopic (—á–µ—Ä–µ–∑ SentenceTransformers).
#  ‚Ä¢ –°—Ç—Ä–æ–∏—Ç —Ç–µ–º—ã –∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç —Ç–æ–ø–æ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–π.
#  ‚Ä¢ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ CSV –∏ –º–æ–¥–µ–ª—å




